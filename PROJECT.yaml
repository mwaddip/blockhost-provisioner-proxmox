# Blockhost Provisioner Project Manifest
# Machine-readable overview for integration and automation
# Last updated: 2026-02-09 (root agent QM action module, provisioner abstraction)

name: blockhost-provisioner-proxmox
version: "0.2.0"
description: |
  Terraform-based Proxmox VM automation with NFT web3 authentication.
  Creates Debian 12 VMs from a cloud-init template that includes libpam-web3
  for Ethereum wallet-based SSH login.

  This package provides VM provisioning scripts. Configuration and database
  modules are provided by the blockhost-common package.

# Primary entry points for integration
entry_points:
  create_vm:
    script: scripts/vm-generator.py
    description: Generate and optionally apply Terraform config for a new VM
    required_args:
      - name: vm_name
        position: 1
        description: Name for the VM (e.g., web-001)
      - name: --owner-wallet
        description: Ethereum wallet address to receive the access NFT
    optional_args:
      - name: --purpose
        description: Description/purpose of the VM
      - name: --cpu
        type: int
        default: 1
        description: Number of CPU cores
      - name: --memory
        type: int
        default: 512
        description: Memory in MB
      - name: --disk
        type: int
        default: 10
        description: Disk size in GB
      - name: --tags
        type: list
        description: Tags for the VM
      - name: --expiry-days
        type: int
        default: 30
        description: Days until VM expires
      - name: --apply
        type: flag
        description: Run terraform apply after generating config
      - name: --mock
        type: flag
        description: Use mock database for testing
      - name: --user-signature
        description: User's decrypted signature from subscription system (hex)
      - name: --public-secret
        description: Message the user signed during subscription
      - name: --no-mint
        type: flag
        aliases: [--skip-mint]
        description: Skip NFT minting (engine handles minting separately)
      - name: --no-web3
        type: flag
        description: Disable web3 auth, use standard cloud-init
      - name: --cloud-init
        description: Cloud-init template name (default nft-auth when web3 enabled)
      - name: --cloud-init-content
        description: Path to pre-rendered cloud-init YAML file (overrides template rendering)
      - name: --ip
        description: Specific IPv4 address (otherwise auto-allocated)
      - name: --ipv6
        description: Specific IPv6 address (otherwise auto-allocated from broker pool)
      - name: --node
        description: Proxmox node name (default from terraform.tfvars or 'pve')
      - name: --disk-datastore
        description: Datastore for VM disk (default from terraform.tfvars proxmox_storage or 'local')
      - name: --cloudinit-datastore
        description: Datastore for cloud-init (default from terraform.tfvars or 'local')
    outputs:
      - Generated .tf.json file path
      - Allocated VMID
      - Allocated IPv4 address
      - Allocated IPv6 address (if broker pool configured)
      - Reserved NFT token ID (if web3 enabled)
      - JSON summary as last stdout line (when --apply succeeds)
    json_output:
      description: |
        On successful --apply, the last line of stdout is a JSON object for engine consumption.
        Format: {"status":"ok","vm_name":"...","ip":"...","ipv6":"...","vmid":N,"nft_token_id":N,"username":"..."}
        Fields ipv6 and nft_token_id may be null.
    example: |
      # Engine-driven: create VM, skip minting
      python3 scripts/vm-generator.py web-001 \
        --owner-wallet 0x1234...abcd \
        --apply --no-mint

      # Legacy: create VM and mint inline
      python3 scripts/vm-generator.py web-001 \
        --owner-wallet 0x1234...abcd \
        --purpose "production web server" \
        --cpu 2 --memory 2048 \
        --apply

  garbage_collection:
    script: scripts/vm-gc.py
    description: |
      Two-phase VM garbage collection:
      Phase 1 (Suspend): Shuts down expired VMs, preserves disk data
      Phase 2 (Destroy): Destroys suspended VMs past grace period
    optional_args:
      - name: --execute
        type: flag
        description: Actually perform actions (default is dry run)
      - name: --suspend-only
        type: flag
        description: Only run Phase 1 (suspend expired VMs)
      - name: --destroy-only
        type: flag
        description: Only run Phase 2 (destroy past-grace VMs)
      - name: --grace-days
        type: int
        description: Override grace period from config (days after expiry before destroy)
      - name: --mock
        type: flag
        description: Use mock database for testing
      - name: --verbose
        type: flag
        description: Verbose output
    systemd_timer:
      timer_file: systemd/blockhost-gc.timer
      service_file: systemd/blockhost-gc.service
      schedule: Daily at 2:00 AM with up to 30 minutes randomized delay
    example: |
      # Dry run both phases
      python3 scripts/vm-gc.py

      # Execute both phases
      python3 scripts/vm-gc.py --execute

      # Only suspend expired VMs
      python3 scripts/vm-gc.py --execute --suspend-only

      # Check timer status
      systemctl status blockhost-gc.timer

  resume_vm:
    script: scripts/vm-resume.py
    description: Resume a suspended VM, restoring it to active status
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to resume
    optional_args:
      - name: --extend-days
        type: int
        description: Extend expiry by this many days (default from config)
      - name: --mock
        type: flag
        description: Use mock database for testing
      - name: --dry-run
        type: flag
        description: Show what would be done without making changes
    example: |
      # Resume a suspended VM
      blockhost-vm-resume myvm

      # Resume and extend expiry by 30 days
      blockhost-vm-resume myvm --extend-days 30

  build_template:
    script: scripts/build-template.sh
    description: Build and deploy Debian 12 template with libpam-web3 to Proxmox
    environment_vars:
      - name: PROXMOX_HOST
        default: root@ix
        description: SSH target for Proxmox host
      - name: TEMPLATE_VMID
        default: "9001"
        description: VMID for the template
      - name: STORAGE
        default: local-lvm
        description: Proxmox storage for VM disks
      - name: LIBPAM_WEB3_DEB
        default: ~/projects/libpam-web3/packaging/libpam-web3_0.2.0_amd64.deb
        description: Path to libpam-web3 .deb package
    prerequisites:
      - libguestfs-tools (virt-customize)
      - SSH access to Proxmox host
      - libpam-web3 .deb package
    example: |
      PROXMOX_HOST=root@pve ./scripts/build-template.sh

  destroy_vm:
    script: scripts/vm-destroy.sh
    description: Destroy a VM — removes terraform config, applies, cleans up IPv6 route
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to destroy
    example: |
      blockhost-vm-destroy web-001

  start_vm:
    script: scripts/vm-start.sh
    description: Start a VM via root agent (qm start)
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to start

  stop_vm:
    script: scripts/vm-stop.sh
    description: Gracefully shut down a VM via root agent (qm shutdown)
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to stop

  kill_vm:
    script: scripts/vm-kill.sh
    description: Force-stop a VM via root agent (qm stop)
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to kill

  vm_status:
    script: scripts/vm-status.sh
    description: Print the status of a VM (active, suspended, destroyed, unknown)
    required_args:
      - name: vm_name
        position: 1
        description: Name of the VM to query

  vm_list:
    script: scripts/vm-list.sh
    description: List all VMs (text or JSON output)
    optional_args:
      - name: --json
        type: flag
        description: Output as JSON array
    example: |
      blockhost-vm-list
      blockhost-vm-list --json

  vm_metrics:
    script: scripts/vm-metrics.sh
    description: VM metrics (stub — future functionality)
    note: Exits 0, not yet implemented

  vm_throttle:
    script: scripts/vm-throttle.sh
    description: VM resource throttling (stub — future functionality)
    note: Exits 0, not yet implemented

  provisioner_detect:
    script: scripts/provisioner-detect.sh
    description: Detect whether this host has Proxmox VE (checks for pvesh)
    outputs:
      - Exit code 0 if Proxmox detected, 1 otherwise

# Provisioner manifest (engine integration)
provisioner_manifest:
  file: provisioner.json
  install_path: /usr/share/blockhost/provisioner.json
  description: |
    Declares what commands this provisioner provides, how its wizard integrates,
    what finalization steps it owns, and where its root agent actions live.
    One active provisioner per host. Read by blockhost-engine's ProvisionerDispatcher.
  commands:
    create: blockhost-vm-create
    destroy: blockhost-vm-destroy
    start: blockhost-vm-start
    stop: blockhost-vm-stop
    kill: blockhost-vm-kill
    status: blockhost-vm-status
    list: blockhost-vm-list
    metrics: blockhost-vm-metrics (stub)
    throttle: blockhost-vm-throttle (stub)
    build-template: blockhost-build-template
    gc: blockhost-vm-gc
    resume: blockhost-vm-resume
  setup:
    detect: blockhost-provisioner-detect
    wizard_module: blockhost.provisioner_proxmox.wizard
    finalization_steps: [token, terraform, db_config, bridge, template]
  root_agent_actions: /usr/share/blockhost/root-agent-actions/qm.py

# First-boot hook (called by main first-boot.sh)
first_boot_hook:
  file: provisioner-hooks/first-boot.sh
  install_path: /usr/share/blockhost/provisioner-hooks/first-boot.sh
  description: |
    Installs Proxmox VE, Terraform, and libguestfs-tools at first boot.
    Called by the main first-boot.sh via the manifest's setup.first_boot_hook path.
    Uses step markers in STATE_DIR for idempotent execution.
  environment:
    STATE_DIR: Step marker directory (default /var/lib/blockhost)
    LOG_FILE: Log file path (default /var/log/blockhost-firstboot.log)
  steps:
    - hostname: Configure /etc/hosts for Proxmox (replace 127.0.1.1 with real IP)
    - proxmox: Install proxmox-ve package from PVE repository
    - terraform: Install Terraform from HashiCorp repository + libguestfs-tools
  step_markers:
    - .step-hostname
    - .step-proxmox
    - .step-terraform

# Root agent action module (loaded by root agent daemon)
root_agent_actions:
  file: root-agent-actions/qm.py
  install_path: /usr/share/blockhost/root-agent-actions/qm.py
  description: |
    Proxmox QM actions for the root agent daemon. Loaded as a plugin module
    at daemon startup. Provides privileged VM operations (create, start, stop,
    destroy, set config, import disk, convert to template).
  actions:
    qm-start: Start a VM (qm start <vmid>)
    qm-stop: Force-stop a VM (qm stop <vmid>)
    qm-shutdown: Gracefully shut down a VM (qm shutdown <vmid>)
    qm-destroy: Destroy a VM and purge disk (qm destroy <vmid> --purge)
    qm-create: Create a VM with validated arguments
    qm-importdisk: Import a disk image into a VM
    qm-set: Set VM configuration with allowlisted keys
    qm-template: Convert a VM to a template
  security:
    - All vmids validated against VMID_MIN/VMID_MAX range
    - qm-create args restricted to QM_CREATE_ALLOWED_ARGS allowlist
    - qm-set keys restricted to QM_SET_ALLOWED_KEYS allowlist
    - qm-importdisk image paths restricted to /var/lib/blockhost/ and /tmp/
    - qm-importdisk storage names validated against STORAGE_RE regex

# Wizard plugin (loaded by main app's installer)
wizard_plugin:
  module: blockhost.provisioner_proxmox.wizard
  source: blockhost/provisioner_proxmox/wizard.py
  install_path: /usr/lib/python3/dist-packages/blockhost/provisioner_proxmox/
  provides:
    - blueprint: Flask Blueprint registered by main app at startup
    - get_ui_params(session_data): Returns provisioner-specific UI parameters for templates
    - get_finalization_steps(): Returns [(step_id, display_name, callable[, hint]), ...]
    - get_summary_data(session_data): Returns provisioner-specific summary dict
    - get_summary_template(): Returns template name for summary section
  finalization_steps:
    - token: Create Proxmox API token via pveum
    - terraform: Configure bpg/proxmox provider, generate SSH key, terraform init
    - db_config: Write /etc/blockhost/db.yaml with provisioner runtime config
    - bridge: Ensure network bridge exists (pvesh create, fallback to ip commands)
    - template: Build VM template with libpam-web3 via build-template.sh
  templates:
    - provisioner_proxmox/proxmox.html: Wizard step for Proxmox configuration
    - provisioner_proxmox/summary_section.html: Summary page fragment

# Python module API (for programmatic use)
python_api:
# Configuration and database modules are provided by blockhost-common
# See: from blockhost.config import load_db_config, load_web3_config
# See: from blockhost.vm_db import get_database, VMDatabase, MockVMDatabase
external_modules:
  blockhost_common:
    package: blockhost-common
    provides:
      - blockhost.config: Path constants, config loading (load_db_config, load_web3_config, load_broker_allocation)
      - blockhost.vm_db: Database classes (VMDatabase, MockVMDatabase, get_database) with allocate_ip and allocate_ipv6
      - blockhost.root_agent: Root agent client (qm_start, qm_stop, qm_shutdown, qm_destroy, ip6_route_add, ip6_route_del, RootAgentError)
      - blockhost.cloud_init: Template rendering (render_cloud_init, find_template, list_templates)
    cloud_init_templates:
      - nft-auth.yaml (NFT authentication with web3-auth-svc)
      - webserver.yaml (basic webserver with nginx)
      - devbox.yaml (development environment with common tools)
    config_files:
      - /etc/blockhost/db.yaml
      - /etc/blockhost/web3-defaults.yaml
      - /etc/blockhost/blockhost.yaml
      - /etc/blockhost/broker-allocation.json  # Created by blockhost-broker client, provides IPv6 prefix
      - /var/lib/blockhost/terraform/terraform.tfvars  # Proxmox node, datastore settings

# Cloud-init templates are shipped by blockhost-common
# See external_modules.blockhost_common.cloud_init_templates above

# External dependencies
dependencies:
  packages:
    - blockhost-common: Config, database, and root agent modules (including IPv6 allocation)
    - blockhost-broker: IPv6 tunnel broker (broker-client creates /etc/blockhost/broker-allocation.json)
    - libpam-web3-tools: pam_web3_tool CLI, signing page (served from local filesystem)
  system:
    - terraform: ">=1.0"
    - libguestfs-tools: For virt-customize
    - foundry: For cast CLI (used by engine's mint_nft)
  python:
    - pyyaml: YAML parsing (from blockhost-common)
  proxmox:
    - provider: bpg/proxmox >=0.50.0
    - template_vmid: 9001 (Debian 12 with libpam-web3)

# Integration notes
integration:
  as_package:
    description: |
      This package is designed to be installed alongside blockhost-common.
      Install both packages, then configure /etc/blockhost/ config files.
    setup_steps:
      - Install blockhost-common package (provides config modules)
      - Install blockhost-provisioner-proxmox package (provides this package)
      - Install libpam-web3-tools package (provides signing page)
      - Run init-server.sh from blockhost-engine to generate keys and config
      - Configure /etc/blockhost/db.yaml with terraform_dir path
      - Configure /etc/blockhost/web3-defaults.yaml with contract details
      - Run build-template.sh to deploy template to Proxmox
    usage: |
      # Create VM with web3 auth
      blockhost-vm-create myvm --owner-wallet 0x... --apply

  terraform_state:
    location: Configured via terraform_dir in /etc/blockhost/db.yaml
    note: Generated .tf.json files are written to terraform_dir

# Privilege separation
privilege_separation:
  description: |
    Scripts run as the unprivileged 'blockhost' user. Privileged operations
    (qm commands, ip route management) are delegated to a root agent daemon
    via blockhost.root_agent client functions. Terraform runs directly as
    blockhost user (HTTP API auth, working dir owned by blockhost).
  unprivileged_user: blockhost
  root_agent_actions_module: root-agent-actions/qm.py
  root_agent_operations:
    - qm_start: Start a VM
    - qm_stop: Force-stop a VM
    - qm_shutdown: Gracefully shut down a VM
    - qm_destroy: Destroy a VM
    - qm_create: Create a VM with validated arguments
    - qm_importdisk: Import disk image into a VM
    - qm_set: Set VM configuration (allowlisted keys)
    - qm_template: Convert a VM to a template
    - ip6_route_add: Add IPv6 host route (shipped by blockhost-common)
    - ip6_route_del: Remove IPv6 host route (shipped by blockhost-common)
  direct_as_blockhost:
    - terraform: HTTP API auth, terraform_dir owned by blockhost
    - cast: Reads deployer key via group perm 0640
    - pam_web3_tool: encrypt-symmetric for connection details
  exceptions:
    - build-template.sh: Runs during installer finalization when process IS root

# VM Status Lifecycle
vm_lifecycle:
  statuses:
    - active: VM is running and accessible
    - suspended: VM shut down (expired but within grace period), disk preserved
    - destroyed: VM and disk removed, database entry marked
  transitions:
    - "active -> suspended": Via vm-gc.py Phase 1 (when VM expires)
    - "suspended -> active": Via vm-resume.py (when subscription extended)
    - "suspended -> destroyed": Via vm-gc.py Phase 2 (when grace period ends)
  grace_period:
    description: |
      After a VM expires, it enters a grace period before destruction.
      During this time, the VM is suspended (shut down but disk preserved).
      Users can extend their subscription to resume the VM.
    config_key: gc_grace_days (in /etc/blockhost/db.yaml)
    default: 7 days

# Data flow
workflow:
  vm_creation:
    steps:
      - 1: Get next NFT token ID from contract totalSupply() (fallback to database)
      - 2: Reserve token ID in database (requires blockhost-common token_id parameter)
      - 3: Allocate VMID, IPv4 and IPv6 (db.allocate_vmid, db.allocate_ip, db.allocate_ipv6)
      - 4: Compute IPv6 gateway from broker prefix (first host address, offset +1)
      - 5: Load Proxmox settings from terraform.tfvars (node, datastores)
      - 6: |
          Use --cloud-init-content if provided, otherwise render cloud-init template with variables:
          VM_NAME, VM_IP, VM_IPV6, SIGNING_HOST, SIGNING_DOMAIN, USERNAME,
          NFT_TOKEN_ID, CHAIN_ID, NFT_CONTRACT, RPC_URL, OTP_LENGTH, OTP_TTL, SECRET_KEY, SSH_KEYS.
          SIGNING_DOMAIN is the broker DNS FQDN ({hex_offset}.{dns_zone}) when dns_zone is present,
          empty string otherwise. SIGNING_HOST is SIGNING_DOMAIN when available, else [IPv6] or IPv4.
      - 7: Generate .tf.json config (with dual-stack IPv4/IPv6, datastore_id, ipv6_gateway)
      - 8: Run terraform apply (if --apply)
      - 9: Add IPv6 host route via root agent (ip6_route_add <ipv6>/128 vmbr0)
      - 10: If not --no-mint and --user-signature provided, encrypt + mint NFT (legacy mode)
      - 11: Print JSON summary as last stdout line
    on_failure:
      - Mark NFT token as failed (guarded against ValueError if not reserved)
      - VM not created, resources released
    blockhost_common_requirements: |
      The provisioner requires these blockhost-common features:
      - reserve_nft_token_id(vm_name, token_id=None): Accept specific token ID
      - register_vm(..., ipv6=None): Store IPv6 address in VM record
      - VM records must include ipv6_address field for route cleanup
      - blockhost.root_agent: Client for privileged operations (qm_*, ip6_route_*)
    ipv6_notes:
      description: |
        IPv6 addresses are allocated from the broker pool (/etc/blockhost/broker-allocation.json).
        If the broker allocation file exists, VMs get both private IPv4 and public IPv6.
        The NFT encrypted connection details use IPv6 (public) when available.
        When broker-allocation.json includes a dns_zone field, the broker runs authoritative
        DNS mapping {hex_offset}.{dns_zone} → {prefix}::{hex_offset}. The VM's FQDN is
        derived as SIGNING_DOMAIN and used as SIGNING_HOST for the cloud-init template.
      gateway: |
        IPv6 gateway is computed as the first host address in the /120 prefix (offset +1).
        This is where the Proxmox host has its address on vmbr0.
      routing: |
        After VM creation, a /128 host route is added via root agent (ip6_route_add)
        to direct inbound traffic via vmbr0. This route is removed when the VM is
        destroyed via vm-gc.py (ip6_route_del).
    encryption_workflow:
      description: |
        When --user-signature is provided, connection details are ECIES-encrypted
        and stored on-chain in the NFT's userEncrypted field.
        The user can later decrypt them by re-signing the same message.
      encryption_method: AES-256-GCM with key = keccak256(user_signature)
      encrypted_data: '{"hostname": "<IPv6 or IPv4>", "port": 22, "username": "<user>"}'
